{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "la0tLUylPEJj",
        "outputId": "90535dd6-f65e-4128-a86b-9d8b36da8653"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/mariaherrerot/ddrdataset?dataset_version_number=1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2.98G/2.98G [01:19<00:00, 40.3MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/mariaherrerot/ddrdataset/versions/1\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"mariaherrerot/ddrdataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFLi5_MdYEWE",
        "outputId": "8fc79171-acf2-44ed-8f0b-2bf8cfe42f8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 292ms/step - accuracy: 0.4152 - loss: 1.9933 - val_accuracy: 0.6228 - val_loss: 1.1332\n",
            "Epoch 2/15\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 244ms/step - accuracy: 0.5400 - loss: 1.3379 - val_accuracy: 0.6295 - val_loss: 1.1553\n",
            "Epoch 3/15\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 243ms/step - accuracy: 0.5831 - loss: 1.2226 - val_accuracy: 0.6327 - val_loss: 1.1671\n",
            "Epoch 4/15\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 241ms/step - accuracy: 0.5921 - loss: 1.2040 - val_accuracy: 0.6248 - val_loss: 1.1523\n",
            "Epoch 16/20\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 289ms/step - accuracy: 0.5415 - loss: 1.2180 - val_accuracy: 0.6228 - val_loss: 1.0042 - learning_rate: 1.0000e-06\n",
            "Epoch 17/20\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 244ms/step - accuracy: 0.5572 - loss: 1.1520 - val_accuracy: 0.6303 - val_loss: 0.9930 - learning_rate: 1.0000e-06\n",
            "Epoch 18/20\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 251ms/step - accuracy: 0.5775 - loss: 1.1030 - val_accuracy: 0.6311 - val_loss: 0.9843 - learning_rate: 1.0000e-06\n",
            "Epoch 19/20\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 246ms/step - accuracy: 0.5803 - loss: 1.1071 - val_accuracy: 0.6435 - val_loss: 0.9733 - learning_rate: 1.0000e-06\n",
            "Epoch 20/20\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 239ms/step - accuracy: 0.5800 - loss: 1.0846 - val_accuracy: 0.6495 - val_loss: 0.9613 - learning_rate: 1.0000e-06\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 117ms/step\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 82ms/step\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 101ms/step\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 316ms/step\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 137ms/step\n",
            "SVM val acc: 0.7445109780439122\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 66ms/step\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 66ms/step\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 59ms/step\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 63ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step\n",
            "Stacked ensemble test acc: 0.7581803671189146\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model, regularizers, callbacks, mixed_precision\n",
        "from tensorflow.keras.applications import ResNet50, VGG19\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Enable mixed precision\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# Paths & params\n",
        "BASE_DIR      = \"/root/.cache/kagglehub/datasets/mariaherrerot/ddrdataset/versions/1\"\n",
        "CSV_PATH      = os.path.join(BASE_DIR, \"DR_grading.csv\")\n",
        "IMG_DIR       = os.path.join(BASE_DIR, \"DR_grading\", \"DR_grading\")\n",
        "IMG_SIZE      = 224\n",
        "BATCH_SIZE    = 64\n",
        "INITIAL_LR    = 1e-4\n",
        "FINE_LR       = 1e-6\n",
        "INITIAL_EPOCHS= 15\n",
        "FINE_EPOCHS   = 5\n",
        "AUTOTUNE      = tf.data.AUTOTUNE\n",
        "WEIGHT_DECAY  = 1e-4\n",
        "\n",
        "# Load & standardize CSV\n",
        "raw = pd.read_csv(CSV_PATH)\n",
        "for c in ['image','filename','ID']:\n",
        "    if c in raw.columns:\n",
        "        raw.rename(columns={c:'id_code'}, inplace=True)\n",
        "        break\n",
        "for c in ['level','label','Label']:\n",
        "    if c in raw.columns:\n",
        "        raw.rename(columns={c:'diagnosis'}, inplace=True)\n",
        "        break\n",
        "raw['id_code'] = raw['id_code'].str.strip().str.replace(r'\\.jpg$', '', regex=True)\n",
        "# map image paths\n",
        "paths = glob.glob(os.path.join(IMG_DIR, \"*.jpg\"))\n",
        "path_map = {os.path.splitext(os.path.basename(p))[0]: p for p in paths}\n",
        "raw['image_path'] = raw['id_code'].map(path_map)\n",
        "df = raw.dropna(subset=['image_path']).reset_index(drop=True)\n",
        "NUM_CLASSES = df['diagnosis'].nunique()\n",
        "\n",
        "# Split\n",
        "train_val, test_df = train_test_split(df, test_size=0.10, stratify=df['diagnosis'], random_state=42)\n",
        "val_ratio = 0.20 / 0.90\n",
        "train_df, val_df = train_test_split(train_val, test_size=val_ratio,\n",
        "                                     stratify=train_val['diagnosis'], random_state=42)\n",
        "\n",
        "# Dataset builder\n",
        "def build_ds(df, augment=False, shuffle=False, repeat=False):\n",
        "    ds = tf.data.Dataset.from_tensor_slices((df['image_path'].values,\n",
        "                                             df['diagnosis'].values))\n",
        "    if shuffle: ds = ds.shuffle(len(df))\n",
        "    if repeat:  ds = ds.repeat()\n",
        "    def _parse(path, label):\n",
        "        img = tf.io.read_file(path)\n",
        "        img = tf.image.decode_jpeg(img, channels=3)\n",
        "        img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
        "        img = tf.keras.applications.resnet.preprocess_input(img)\n",
        "        return img, tf.one_hot(label, NUM_CLASSES)\n",
        "    ds = ds.map(_parse, num_parallel_calls=AUTOTUNE)\n",
        "    if augment:\n",
        "        aug = tf.keras.Sequential([\n",
        "            layers.RandomFlip('horizontal_and_vertical'),\n",
        "            layers.RandomRotation(0.2),\n",
        "            layers.RandomZoom(0.1, 0.1)\n",
        "        ])\n",
        "        ds = ds.map(lambda x, y: (aug(x, training=True), y), num_parallel_calls=AUTOTUNE)\n",
        "    return ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "\n",
        "train_ds = build_ds(train_df, augment=True, shuffle=True, repeat=True)\n",
        "val_ds   = build_ds(val_df)\n",
        "test_ds  = build_ds(test_df)\n",
        "\n",
        "# 1) ResNet50 model\n",
        "def make_resnet_model():\n",
        "    base = ResNet50(weights='imagenet', include_top=False,\n",
        "                    input_shape=(IMG_SIZE,IMG_SIZE,3))\n",
        "    base.trainable = False\n",
        "    inp = layers.Input((IMG_SIZE,IMG_SIZE,3))\n",
        "    x = base(inp, training=False)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(WEIGHT_DECAY))(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    out = layers.Dense(NUM_CLASSES, activation='softmax', dtype='float32')(x)\n",
        "    model = Model(inp, out)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(INITIAL_LR),\n",
        "                  loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "resnet_model = make_resnet_model()\n",
        "resnet_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=INITIAL_EPOCHS,\n",
        "    steps_per_epoch=len(train_df)//BATCH_SIZE,\n",
        "    callbacks=[callbacks.EarlyStopping('val_loss', patience=3, restore_best_weights=True)]\n",
        ")\n",
        "# Fine-tune last block\n",
        "for layer in resnet_model.layers[1].layers[-50:]:\n",
        "    if not isinstance(layer, layers.BatchNormalization):\n",
        "        layer.trainable = True\n",
        "resnet_model.compile(optimizer=tf.keras.optimizers.Adam(FINE_LR),\n",
        "                     loss='categorical_crossentropy',\n",
        "                     metrics=['accuracy'])\n",
        "resnet_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=INITIAL_EPOCHS+FINE_EPOCHS,\n",
        "    initial_epoch=INITIAL_EPOCHS,\n",
        "    steps_per_epoch=len(train_df)//BATCH_SIZE,\n",
        "    callbacks=[\n",
        "        callbacks.ReduceLROnPlateau('val_loss',factor=0.5,patience=2),\n",
        "        callbacks.EarlyStopping('val_loss',patience=3,restore_best_weights=True)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 2) VGG19 feature extractor\n",
        "def make_vgg_feat():\n",
        "    base = VGG19(weights='imagenet', include_top=False,\n",
        "                 input_shape=(IMG_SIZE,IMG_SIZE,3))\n",
        "    base.trainable = False\n",
        "    inp = layers.Input((IMG_SIZE,IMG_SIZE,3))\n",
        "    x = base(inp, training=False)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    return Model(inp, x, name='vgg19_feat')\n",
        "\n",
        "vgg_feat = make_vgg_feat()\n",
        "\n",
        "# 3) Feature extraction\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "def extract_feats(model, df):\n",
        "    ds = build_ds(df)\n",
        "    steps = len(df)//BATCH_SIZE + 1\n",
        "    feats = model.predict(ds, steps=steps, verbose=1)\n",
        "    labels = df['diagnosis'].values\n",
        "    return feats, labels\n",
        "\n",
        "X_tr_rn, y_tr    = extract_feats(resnet_model, train_df)\n",
        "X_tr_vg, _       = extract_feats(vgg_feat, train_df)\n",
        "X_val_rn, y_val  = extract_feats(resnet_model, val_df)\n",
        "X_val_vg, _      = extract_feats(vgg_feat, val_df)\n",
        "X_test_rn, y_test= extract_feats(resnet_model, test_df)\n",
        "X_test_vg, _     = extract_feats(vgg_feat, test_df)\n",
        "\n",
        "# Stack features for SVM\n",
        "X_tr = np.hstack([X_tr_rn, X_tr_vg])\n",
        "X_val= np.hstack([X_val_rn, X_val_vg])\n",
        "X_te = np.hstack([X_test_rn, X_test_vg])\n",
        "\n",
        "# 4) Train SVM on stacked features\n",
        "svm = make_pipeline(StandardScaler(), SVC(kernel='rbf', probability=True, C=10))\n",
        "svm.fit(X_tr, y_tr)\n",
        "print(\"SVM val acc:\", svm.score(X_val, y_val))\n",
        "\n",
        "# 5) Build meta-features from all three models\n",
        "p_tr_rn = resnet_model.predict(build_ds(train_df), steps=len(train_df)//BATCH_SIZE+1)\n",
        "p_tr_vg = vgg_feat.predict(build_ds(train_df), steps=len(train_df)//BATCH_SIZE+1)\n",
        "p_tr_vg = tf.keras.activations.softmax(layers.Dense(NUM_CLASSES, dtype='float32')(p_tr_vg)).numpy()\n",
        "p_tr_svm= svm.predict_proba(X_tr)\n",
        "meta_tr = np.hstack([p_tr_rn, p_tr_vg, p_tr_svm])\n",
        "\n",
        "p_val_rn= resnet_model.predict(build_ds(val_df), steps=len(val_df)//BATCH_SIZE+1)\n",
        "p_val_vg= vgg_feat.predict(build_ds(val_df), steps=len(val_df)//BATCH_SIZE+1)\n",
        "p_val_vg= tf.keras.activations.softmax(layers.Dense(NUM_CLASSES, dtype='float32')(p_val_vg)).numpy()\n",
        "p_val_svm= svm.predict_proba(X_val)\n",
        "\n",
        "meta_val= np.hstack([p_val_rn, p_val_vg, p_val_svm])\n",
        "\n",
        "# 6) Meta-learner: Logistic Regression\n",
        "meta_clf = LogisticRegression(multi_class='multinomial', max_iter=2000)\n",
        "meta_clf.fit(meta_tr, y_tr)\n",
        "print(\"Stacked ensemble test acc:\", meta_clf.score(np.hstack([resnet_model.predict(build_ds(test_df), steps=len(test_df)//BATCH_SIZE+1),tf.keras.activations.softmax(layers.Dense(NUM_CLASSES, dtype='float32')(vgg_feat.predict(build_ds(test_df), steps=len(test_df)//BATCH_SIZE+1))).numpy(),svm.predict_proba(X_te)]),y_test))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
